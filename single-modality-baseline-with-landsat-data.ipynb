{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49f73eb",
   "metadata": {
    "papermill": {
     "duration": 0.006251,
     "end_time": "2024-05-05T21:00:58.823056",
     "exception": false,
     "start_time": "2024-05-05T21:00:58.816805",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Simple baseline with Landsat Cubes â€” ResNet18 + Binary Cross Entropy [0.26424]\n\nTo demonstrate the potential of other data such as Landsat cubes, we provide a\nstraightforward baseline that is baseline on a modified ResNet18 and Binary\nCross Entropy but still ranks highly on the leaderboard. The model itself should\nlearn the relationship between the location [R, G, B, NIR, SWIR1, and SWIR2]\nvalue at a given location and its species composition.\n\nConsidering the significant extent for enhancing performance of this baseline,\nwe encourage you to experiment with various techniques, architectures, losses,\netc.\n\n#### **Have Fun!**"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d8d70b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:07.29831Z",
     "start_time": "2024-04-30T21:25:05.354584Z"
    },
    "execution": {
     "iopub.execute_input": "2025-03-13T20:16:59.136787Z",
     "iopub.status.busy": "2025-03-13T20:16:59.136493Z",
     "iopub.status.idle": "2025-03-13T20:17:06.771058Z",
     "shell.execute_reply": "2025-03-13T20:17:06.770364Z",
     "shell.execute_reply.started": "2025-03-13T20:16:59.136757Z"
    },
    "papermill": {
     "duration": 7.910804,
     "end_time": "2024-05-05T21:01:06.739690",
     "exception": false,
     "start_time": "2024-05-05T21:00:58.828886",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54ede5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-01T13:30:07.054038Z",
     "iopub.status.busy": "2024-05-01T13:30:07.053659Z",
     "iopub.status.idle": "2024-05-01T13:30:07.058148Z",
     "shell.execute_reply": "2024-05-01T13:30:07.057269Z",
     "shell.execute_reply.started": "2024-05-01T13:30:07.054008Z"
    },
    "papermill": {
     "duration": 0.005438,
     "end_time": "2024-05-05T21:01:06.751219",
     "exception": false,
     "start_time": "2024-05-05T21:01:06.745781",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Data description\n\nSatellite time series data includes over 20 years of Landsat satellite imagery\nextracted from [Ecodatacube](https://stac.ecodatacube.eu/). The data was\nacquired through the Landsat satellite program and pre-processed by Ecodatacube\nto produce raster files scaled to the entire European continent and projected\ninto a unique CRS.\n\nSince the original rasters require a high amount of disk space, we extracted the\ndata points from each spectral band corresponding to all PA and PO locations\n(i.e., GPS coordinates) and aggregated them in (i) CSV files and (ii) data cubes\nas tensor objects. Each data point corresponds to the mean value of Landsat's\nobservations at the given location for three months before the given time; e.g.,\nthe value of a time series element under column 2012_4 will represent the mean\nvalue for that element from October 2012 to December 2012.\n\nIn this notebook, we will work with just the cubes. The cubes are structured as\nfollows. **Shape**: `(n_bands, n_quarters, n_years)` where:\n\n- `n_bands` = 6 comprising [`red`, `green`, `blue`, `nir`, `swir1`, `swir2`]\n- `n_quarters` = 4\n  - _Quarter 1_: December 2 of previous year until March 20 of current year\n    (winter season proxy),\n  - _Quarter 2_: March 21 until June 24 of current year (spring season proxy),\n  - _Quarter 3_: June 25 until September 12 of current year (summer season\n    proxy),\n  - _Quarter 4_: September 13 until December 1 of current year (fall season\n    proxy).\n- `n_years` = 21 (ranging from 2000 to 2020)\n\nThe datacubes can simply be loaded as tensors using PyTorch with the following\ncommand :\n\n```python\nimport torch\ntorch.load('path_to_file.pt')\n```\n\n**References:**\n\n- _Traceability (lineage): This dataset is a seasonally aggregated and gapfilled\n  version of the Landsat GLAD analysis-ready data product presented by Potapov\n  et al., 2020 ( https://doi.org/10.3390/rs12030426 )._\n- _Scientific methodology: The Landsat GLAD ARD dataset was aggregated and\n  harmonized using the eumap python package (available at\n  https://eumap.readthedocs.io/en/latest/ ). The full process of gapfilling and\n  harmonization is described in detail in Witjes et al., 2022 (in review,\n  preprint available at https://doi.org/10.21203/rs.3.rs-561383/v3 )._\n- _Ecodatacube.eu: Analysis-ready open environmental data cube for Europe\n  (https://doi.org/10.21203/rs.3.rs-2277090/v3)._"
  },
  {
   "cell_type": "markdown",
   "id": "f758ba3a",
   "metadata": {
    "papermill": {
     "duration": 0.005422,
     "end_time": "2024-05-05T21:01:06.762205",
     "exception": false,
     "start_time": "2024-05-05T21:01:06.756783",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Prepare custom dataset loader\n\nWe have to sloightly update the Dataset to provide the relevant data in the\nappropriate format."
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba67bb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.627928Z",
     "start_time": "2024-04-30T21:25:32.612131Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T20:17:06.772432Z",
     "iopub.status.busy": "2025-03-13T20:17:06.771946Z",
     "iopub.status.idle": "2025-03-13T20:17:06.781139Z",
     "shell.execute_reply": "2025-03-13T20:17:06.780321Z",
     "shell.execute_reply.started": "2025-03-13T20:17:06.772403Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022488,
     "end_time": "2024-05-05T21:01:06.790318",
     "exception": false,
     "start_time": "2024-05-05T21:01:06.767830",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "        self.metadata = self.metadata.dropna(subset=\"speciesId\").reset_index(drop=True)\n",
    "        self.metadata[\"speciesId\"] = self.metadata[\"speciesId\"].astype(int)\n",
    "        self.label_dict = (\n",
    "            self.metadata.groupby(\"surveyId\")[\"speciesId\"].apply(list).to_dict()\n",
    "        )\n",
    "\n",
    "        self.metadata = self.metadata.drop_duplicates(subset=\"surveyId\").reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.nan_to_num(\n",
    "            torch.load(\n",
    "                os.path.join(\n",
    "                    self.data_dir,\n",
    "                    f\"GLC25-PA-{self.subset}-landsat-time-series_{survey_id}_cube.pt\",\n",
    "                ),\n",
    "                weights_only=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        species_ids = self.label_dict.get(\n",
    "            survey_id, []\n",
    "        )  # Get list of species IDs for the survey ID\n",
    "        label = torch.zeros(num_classes)  # Initialize label tensor\n",
    "        for species_id in species_ids:\n",
    "            # label_id = self.species_mapping[species_id]  # Get consecutive integer label\n",
    "            label_id = species_id\n",
    "            label[label_id] = (\n",
    "                1  # Set the corresponding class index to 1 for each species\n",
    "            )\n",
    "\n",
    "        # Ensure the sample is in the correct format for the transform\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(\n",
    "                1, 2, 0\n",
    "            )  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            sample = sample.numpy()  # Convert tensor to numpy array\n",
    "            # print(sample.shape)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, label, survey_id\n",
    "\n",
    "\n",
    "class TestDataset(TrainDataset):\n",
    "    def __init__(self, data_dir, metadata, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        survey_id = self.metadata.surveyId[idx]\n",
    "        sample = torch.nan_to_num(\n",
    "            torch.load(\n",
    "                os.path.join(\n",
    "                    self.data_dir,\n",
    "                    f\"GLC25-PA-{self.subset}-landsat_time_series_{survey_id}_cube.pt\",\n",
    "                ),\n",
    "                weights_only=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if isinstance(sample, torch.Tensor):\n",
    "            sample = sample.permute(\n",
    "                1, 2, 0\n",
    "            )  # Change tensor shape from (C, H, W) to (H, W, C)\n",
    "            sample = sample.numpy()\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, survey_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc07d0",
   "metadata": {
    "papermill": {
     "duration": 0.00538,
     "end_time": "2024-05-05T21:01:06.801283",
     "exception": false,
     "start_time": "2024-05-05T21:01:06.795903",
     "status": "completed"
    },
    "tags": []
   },
   "source": "### Load metadata and prepare data loaders"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a1f08d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:34.532017Z",
     "start_time": "2024-04-30T21:25:32.615562Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T20:17:06.782265Z",
     "iopub.status.busy": "2025-03-13T20:17:06.781975Z",
     "iopub.status.idle": "2025-03-13T20:17:11.154722Z",
     "shell.execute_reply": "2025-03-13T20:17:11.154032Z",
     "shell.execute_reply.started": "2025-03-13T20:17:06.782238Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.726171,
     "end_time": "2024-05-05T21:01:12.535279",
     "exception": false,
     "start_time": "2024-05-05T21:01:06.809108",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load Training metadata\n",
    "\n",
    "\"/kaggle/input/geolifeclef-2025/SateliteTimeSeries-Landsat/cubes/PA-train/\"\n",
    "\n",
    "train_data_path = (\n",
    "    \"/kaggle/input/geolifeclef-2025/SateliteTimeSeries-Landsat/cubes/PA-train/\"\n",
    ")\n",
    "train_metadata_path = \"/kaggle/input/geolifeclef-2025/GLC25_PA_metadata_train.csv\"\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "train_dataset = TrainDataset(\n",
    "    train_data_path, train_metadata, subset=\"train\", transform=transform\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4\n",
    ")\n",
    "\n",
    "# Load Test metadata\n",
    "test_data_path = (\n",
    "    \"/kaggle/input/geolifeclef-2025/SateliteTimeSeries-Landsat/cubes/PA-test/\"\n",
    ")\n",
    "test_metadata_path = \"/kaggle/input/geolifeclef-2025/GLC25_PA_metadata_test.csv\"\n",
    "test_metadata = pd.read_csv(test_metadata_path)\n",
    "test_dataset = TestDataset(\n",
    "    test_data_path, test_metadata, subset=\"test\", transform=transform\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dcec3b",
   "metadata": {
    "papermill": {
     "duration": 0.005747,
     "end_time": "2024-05-05T21:01:12.547063",
     "exception": false,
     "start_time": "2024-05-05T21:01:12.541316",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Define and initialize the ModifiedResNet18 model\n\nTo utilize the landsat cubes, which have a shape of [6,4,21] (BANDs, QUARTERs,\nand YEARs), some minor adjustments must be made to the vanilla ResNet-18. It's\nimportant to note that this is just one method for ensuring compatibility with\nthe unusual tensor shape, and experimentation is encouraged."
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d74624",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.014067Z",
     "start_time": "2024-04-30T21:25:31.01006Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T20:17:11.155839Z",
     "iopub.status.busy": "2025-03-13T20:17:11.155602Z",
     "iopub.status.idle": "2025-03-13T20:17:11.163024Z",
     "shell.execute_reply": "2025-03-13T20:17:11.162365Z",
     "shell.execute_reply.started": "2025-03-13T20:17:11.155819Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015915,
     "end_time": "2024-05-05T21:01:12.568601",
     "exception": false,
     "start_time": "2024-05-05T21:01:12.552686",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModifiedResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModifiedResNet18, self).__init__()\n",
    "\n",
    "        self.norm_input = nn.LayerNorm([6, 4, 21])\n",
    "        self.resnet18 = models.resnet18(weights=None)\n",
    "        # We have to modify the first convolutional layer to accept 4 channels instead of 3\n",
    "        self.resnet18.conv1 = nn.Conv2d(\n",
    "            6, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.resnet18.maxpool = nn.Identity()\n",
    "        self.ln = nn.LayerNorm(1000)\n",
    "        self.fc1 = nn.Linear(1000, 2056)\n",
    "        self.fc2 = nn.Linear(2056, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm_input(x)\n",
    "        x = self.resnet18(x)\n",
    "        x = self.ln(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e98ba53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T20:17:11.164887Z",
     "iopub.status.busy": "2025-03-13T20:17:11.164643Z",
     "iopub.status.idle": "2025-03-13T20:17:11.268616Z",
     "shell.execute_reply": "2025-03-13T20:17:11.267695Z",
     "shell.execute_reply.started": "2025-03-13T20:17:11.164866Z"
    },
    "papermill": {
     "duration": 0.058735,
     "end_time": "2024-05-05T21:01:12.632872",
     "exception": false,
     "start_time": "2024-05-05T21:01:12.574137",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    # Set seed for Python's built-in random number generator\n",
    "    torch.manual_seed(seed)\n",
    "    # Set seed for numpy\n",
    "    np.random.seed(seed)\n",
    "    # Set seed for CUDA if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Set cuDNN's random number generator seed for deterministic behavior\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "828330c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.611823Z",
     "start_time": "2024-04-30T21:25:31.607373Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T20:17:11.269858Z",
     "iopub.status.busy": "2025-03-13T20:17:11.269620Z",
     "iopub.status.idle": "2025-03-13T20:17:11.920107Z",
     "shell.execute_reply": "2025-03-13T20:17:11.919307Z",
     "shell.execute_reply.started": "2025-03-13T20:17:11.269838Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.676292,
     "end_time": "2024-05-05T21:01:13.314798",
     "exception": false,
     "start_time": "2024-05-05T21:01:12.638506",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DEVICE = CUDA\n"
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"DEVICE = CUDA\")\n",
    "\n",
    "num_classes = 11255  # Number of all unique classes within the PO and PA data.\n",
    "model = ModifiedResNet18(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648576c4",
   "metadata": {
    "papermill": {
     "duration": 0.005509,
     "end_time": "2024-05-05T21:01:13.326480",
     "exception": false,
     "start_time": "2024-05-05T21:01:13.320971",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Training Loop\n\nNothing special, just a standard Pytorch training loop."
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35d34a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:32.181927Z",
     "start_time": "2024-04-30T21:25:32.177073Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T20:17:11.921189Z",
     "iopub.status.busy": "2025-03-13T20:17:11.920875Z",
     "iopub.status.idle": "2025-03-13T20:17:11.927762Z",
     "shell.execute_reply": "2025-03-13T20:17:11.926976Z",
     "shell.execute_reply.started": "2025-03-13T20:17:11.921159Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.015096,
     "end_time": "2024-05-05T21:01:13.347192",
     "exception": false,
     "start_time": "2024-05-05T21:01:13.332096",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n"
    }
   ],
   "source": [
    "#Hyperparameters;\n",
    "learning_rate = 0.0002;\n",
    "num_epochs = 20;\n",
    "positive_weigh_factor = 1.0;\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate);\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max = 25, verbose = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d1df4d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T21:25:34.536634Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T20:17:11.928704Z",
     "iopub.status.busy": "2025-03-13T20:17:11.928492Z",
     "iopub.status.idle": "2025-03-13T20:40:32.881003Z",
     "shell.execute_reply": "2025-03-13T20:40:32.880021Z",
     "shell.execute_reply.started": "2025-03-13T20:17:11.928686Z"
    },
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1209.843817,
     "end_time": "2024-05-05T21:21:23.196800",
     "exception": false,
     "start_time": "2024-05-05T21:01:13.352983",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training for 20 epochs started.\nEpoch 1/20, Batch 0/1391, Loss: 0.7073062658309937\nEpoch 1/20, Batch 278/1391, Loss: 0.006236988585442305\nEpoch 1/20, Batch 556/1391, Loss: 0.0056686000898480415\nEpoch 1/20, Batch 834/1391, Loss: 0.005640304647386074\nEpoch 1/20, Batch 1112/1391, Loss: 0.005799697246402502\nEpoch 1/20, Batch 1390/1391, Loss: 0.004818603862076998\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 1, 'verbose': True, '_step_count': 2, '_get_lr_called_within_step': False, '_last_lr': [0.0001992114701314478]}\nEpoch 2/20, Batch 0/1391, Loss: 0.005887552630156279\nEpoch 2/20, Batch 278/1391, Loss: 0.005388566758483648\nEpoch 2/20, Batch 556/1391, Loss: 0.005396657157689333\nEpoch 2/20, Batch 834/1391, Loss: 0.004122779238969088\nEpoch 2/20, Batch 1112/1391, Loss: 0.005589140113443136\nEpoch 2/20, Batch 1390/1391, Loss: 0.004609483759850264\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 2, 'verbose': True, '_step_count': 3, '_get_lr_called_within_step': False, '_last_lr': [0.0001968583161128631]}\nEpoch 3/20, Batch 0/1391, Loss: 0.0041245692409574986\nEpoch 3/20, Batch 278/1391, Loss: 0.0047186920419335365\nEpoch 3/20, Batch 556/1391, Loss: 0.005271407309919596\nEpoch 3/20, Batch 834/1391, Loss: 0.00410974258556962\nEpoch 3/20, Batch 1112/1391, Loss: 0.004307948984205723\nEpoch 3/20, Batch 1390/1391, Loss: 0.00440904451534152\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 3, 'verbose': True, '_step_count': 4, '_get_lr_called_within_step': False, '_last_lr': [0.00019297764858882514]}\nEpoch 4/20, Batch 0/1391, Loss: 0.004152368288487196\nEpoch 4/20, Batch 278/1391, Loss: 0.004765757359564304\nEpoch 4/20, Batch 556/1391, Loss: 0.005021973513066769\nEpoch 4/20, Batch 834/1391, Loss: 0.00429628137499094\nEpoch 4/20, Batch 1112/1391, Loss: 0.0046791923232376575\nEpoch 4/20, Batch 1390/1391, Loss: 0.004257913213223219\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 4, 'verbose': True, '_step_count': 5, '_get_lr_called_within_step': False, '_last_lr': [0.00018763066800438636]}\nEpoch 5/20, Batch 0/1391, Loss: 0.004219743888825178\nEpoch 5/20, Batch 278/1391, Loss: 0.004184593912214041\nEpoch 5/20, Batch 556/1391, Loss: 0.003917338792234659\nEpoch 5/20, Batch 834/1391, Loss: 0.004861682653427124\nEpoch 5/20, Batch 1112/1391, Loss: 0.00413505407050252\nEpoch 5/20, Batch 1390/1391, Loss: 0.004820463713258505\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 5, 'verbose': True, '_step_count': 6, '_get_lr_called_within_step': False, '_last_lr': [0.00018090169943749476]}\nEpoch 6/20, Batch 0/1391, Loss: 0.004355051554739475\nEpoch 6/20, Batch 278/1391, Loss: 0.004067073110491037\nEpoch 6/20, Batch 556/1391, Loss: 0.004098962526768446\nEpoch 6/20, Batch 834/1391, Loss: 0.004079385194927454\nEpoch 6/20, Batch 1112/1391, Loss: 0.004509023390710354\nEpoch 6/20, Batch 1390/1391, Loss: 0.004491762258112431\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 6, 'verbose': True, '_step_count': 7, '_get_lr_called_within_step': False, '_last_lr': [0.00017289686274214118]}\nEpoch 7/20, Batch 0/1391, Loss: 0.003569869790226221\nEpoch 7/20, Batch 278/1391, Loss: 0.004551185294985771\nEpoch 7/20, Batch 556/1391, Loss: 0.003452289616689086\nEpoch 7/20, Batch 834/1391, Loss: 0.0036994728725403547\nEpoch 7/20, Batch 1112/1391, Loss: 0.003889090847223997\nEpoch 7/20, Batch 1390/1391, Loss: 0.0035792982671409845\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 7, 'verbose': True, '_step_count': 8, '_get_lr_called_within_step': False, '_last_lr': [0.000163742398974869]}\nEpoch 8/20, Batch 0/1391, Loss: 0.0038705740589648485\nEpoch 8/20, Batch 278/1391, Loss: 0.003934561274945736\nEpoch 8/20, Batch 556/1391, Loss: 0.003961270209401846\nEpoch 8/20, Batch 834/1391, Loss: 0.0038283271715044975\nEpoch 8/20, Batch 1112/1391, Loss: 0.0037701313849538565\nEpoch 8/20, Batch 1390/1391, Loss: 0.004386082757264376\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 8, 'verbose': True, '_step_count': 9, '_get_lr_called_within_step': False, '_last_lr': [0.00015358267949789966]}\nEpoch 9/20, Batch 0/1391, Loss: 0.003563304664567113\nEpoch 9/20, Batch 278/1391, Loss: 0.003745668102055788\nEpoch 9/20, Batch 556/1391, Loss: 0.003988406155258417\nEpoch 9/20, Batch 834/1391, Loss: 0.0037349574267864227\nEpoch 9/20, Batch 1112/1391, Loss: 0.0034932114649564028\nEpoch 9/20, Batch 1390/1391, Loss: 0.003918162547051907\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 9, 'verbose': True, '_step_count': 10, '_get_lr_called_within_step': False, '_last_lr': [0.00014257792915650726]}\nEpoch 10/20, Batch 0/1391, Loss: 0.003195667639374733\nEpoch 10/20, Batch 278/1391, Loss: 0.004163473378866911\nEpoch 10/20, Batch 556/1391, Loss: 0.0041799689643085\nEpoch 10/20, Batch 834/1391, Loss: 0.003584045683965087\nEpoch 10/20, Batch 1112/1391, Loss: 0.0037844993639737368\nEpoch 10/20, Batch 1390/1391, Loss: 0.004121029749512672\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 10, 'verbose': True, '_step_count': 11, '_get_lr_called_within_step': False, '_last_lr': [0.00013090169943749474]}\nEpoch 11/20, Batch 0/1391, Loss: 0.0031481864862143993\nEpoch 11/20, Batch 278/1391, Loss: 0.003381204092875123\nEpoch 11/20, Batch 556/1391, Loss: 0.00304659316316247\nEpoch 11/20, Batch 834/1391, Loss: 0.0031531238928437233\nEpoch 11/20, Batch 1112/1391, Loss: 0.0035744374617934227\nEpoch 11/20, Batch 1390/1391, Loss: 0.00364103214815259\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 11, 'verbose': True, '_step_count': 12, '_get_lr_called_within_step': False, '_last_lr': [0.00011873813145857248]}\nEpoch 12/20, Batch 0/1391, Loss: 0.003446536138653755\nEpoch 12/20, Batch 278/1391, Loss: 0.0032613761723041534\nEpoch 12/20, Batch 556/1391, Loss: 0.002962858648970723\nEpoch 12/20, Batch 834/1391, Loss: 0.0033750468865036964\nEpoch 12/20, Batch 1112/1391, Loss: 0.0035885635297745466\nEpoch 12/20, Batch 1390/1391, Loss: 0.0029967857990413904\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 12, 'verbose': True, '_step_count': 13, '_get_lr_called_within_step': False, '_last_lr': [0.00010627905195293135]}\nEpoch 13/20, Batch 0/1391, Loss: 0.0033996973652392626\nEpoch 13/20, Batch 278/1391, Loss: 0.0033079355489462614\nEpoch 13/20, Batch 556/1391, Loss: 0.00326341250911355\nEpoch 13/20, Batch 834/1391, Loss: 0.003217620775103569\nEpoch 13/20, Batch 1112/1391, Loss: 0.003059278940781951\nEpoch 13/20, Batch 1390/1391, Loss: 0.0032393804285675287\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 13, 'verbose': True, '_step_count': 14, '_get_lr_called_within_step': False, '_last_lr': [9.372094804706867e-05]}\nEpoch 14/20, Batch 0/1391, Loss: 0.0032212911173701286\nEpoch 14/20, Batch 278/1391, Loss: 0.002906790701672435\nEpoch 14/20, Batch 556/1391, Loss: 0.0030187603551894426\nEpoch 14/20, Batch 834/1391, Loss: 0.0031911958940327168\nEpoch 14/20, Batch 1112/1391, Loss: 0.002921965904533863\nEpoch 14/20, Batch 1390/1391, Loss: 0.003097265725955367\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 14, 'verbose': True, '_step_count': 15, '_get_lr_called_within_step': False, '_last_lr': [8.126186854142755e-05]}\nEpoch 15/20, Batch 0/1391, Loss: 0.003110978752374649\nEpoch 15/20, Batch 278/1391, Loss: 0.003097609616816044\nEpoch 15/20, Batch 556/1391, Loss: 0.0028738926630467176\nEpoch 15/20, Batch 834/1391, Loss: 0.003207120578736067\nEpoch 15/20, Batch 1112/1391, Loss: 0.003132155630737543\nEpoch 15/20, Batch 1390/1391, Loss: 0.003435122547671199\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 15, 'verbose': True, '_step_count': 16, '_get_lr_called_within_step': False, '_last_lr': [6.90983005625053e-05]}\nEpoch 16/20, Batch 0/1391, Loss: 0.0029864704702049494\nEpoch 16/20, Batch 278/1391, Loss: 0.003031463362276554\nEpoch 16/20, Batch 556/1391, Loss: 0.0029542609117925167\nEpoch 16/20, Batch 834/1391, Loss: 0.0026932107284665108\nEpoch 16/20, Batch 1112/1391, Loss: 0.0026717844884842634\nEpoch 16/20, Batch 1390/1391, Loss: 0.0032849328126758337\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 16, 'verbose': True, '_step_count': 17, '_get_lr_called_within_step': False, '_last_lr': [5.742207084349274e-05]}\nEpoch 17/20, Batch 0/1391, Loss: 0.0027790649328380823\nEpoch 17/20, Batch 278/1391, Loss: 0.002571661723777652\nEpoch 17/20, Batch 556/1391, Loss: 0.002439607633277774\nEpoch 17/20, Batch 834/1391, Loss: 0.0028898303862661123\nEpoch 17/20, Batch 1112/1391, Loss: 0.002834395505487919\nEpoch 17/20, Batch 1390/1391, Loss: 0.003221041290089488\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 17, 'verbose': True, '_step_count': 18, '_get_lr_called_within_step': False, '_last_lr': [4.6417320502100316e-05]}\nEpoch 18/20, Batch 0/1391, Loss: 0.002624648157507181\nEpoch 18/20, Batch 278/1391, Loss: 0.0028801916632801294\nEpoch 18/20, Batch 556/1391, Loss: 0.002546581905335188\nEpoch 18/20, Batch 834/1391, Loss: 0.0025791276711970568\nEpoch 18/20, Batch 1112/1391, Loss: 0.0026311520487070084\nEpoch 18/20, Batch 1390/1391, Loss: 0.0031620708759874105\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 18, 'verbose': True, '_step_count': 19, '_get_lr_called_within_step': False, '_last_lr': [3.6257601025131026e-05]}\nEpoch 19/20, Batch 0/1391, Loss: 0.0023539504036307335\nEpoch 19/20, Batch 278/1391, Loss: 0.002728695748373866\nEpoch 19/20, Batch 556/1391, Loss: 0.0025000760797411203\nEpoch 19/20, Batch 834/1391, Loss: 0.002408551750704646\nEpoch 19/20, Batch 1112/1391, Loss: 0.0028999641072005033\nEpoch 19/20, Batch 1390/1391, Loss: 0.003016951261088252\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 19, 'verbose': True, '_step_count': 20, '_get_lr_called_within_step': False, '_last_lr': [2.7103137257858868e-05]}\nEpoch 20/20, Batch 0/1391, Loss: 0.002385981846600771\nEpoch 20/20, Batch 278/1391, Loss: 0.002223068382591009\nEpoch 20/20, Batch 556/1391, Loss: 0.0024992970284074545\nEpoch 20/20, Batch 834/1391, Loss: 0.0024494328536093235\nEpoch 20/20, Batch 1112/1391, Loss: 0.0023674112744629383\nEpoch 20/20, Batch 1390/1391, Loss: 0.0023253662511706352\nScheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0002], 'last_epoch': 20, 'verbose': True, '_step_count': 21, '_get_lr_called_within_step': False, '_last_lr': [1.909830056250527e-05]}\n"
    }
   ],
   "source": [
    "print(f\"Training for {num_epochs} epochs started.\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "\n",
    "        pos_weight = (\n",
    "            targets * positive_weigh_factor\n",
    "        )  # All positive weights are equal to 10\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 278 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\"\n",
    "            )\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\"Scheduler:\", scheduler.state_dict())\n",
    "\n",
    "# Save the trained model\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), \"resnet18-with-landsat-cubes.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce07f945",
   "metadata": {
    "papermill": {
     "duration": 0.014707,
     "end_time": "2024-05-05T21:21:23.226855",
     "exception": false,
     "start_time": "2024-05-05T21:21:23.212148",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Test Loop\n\nAgain, nothing special, just a standard inference."
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fd35b61",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-13T20:40:32.882443Z",
     "iopub.status.busy": "2025-03-13T20:40:32.882124Z",
     "iopub.status.idle": "2025-03-13T20:41:05.981664Z",
     "shell.execute_reply": "2025-03-13T20:41:05.980764Z",
     "shell.execute_reply.started": "2025-03-13T20:40:32.882408Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 9.799329,
     "end_time": "2024-05-05T21:21:33.041149",
     "exception": false,
     "start_time": "2024-05-05T21:21:23.241820",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231/231 [00:33<00:00,  6.98it/s]\n"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    surveys = []\n",
    "    top_k_indices = None\n",
    "    for data, surveyID in tqdm.tqdm(test_loader, total=len(test_loader)):\n",
    "        data = data.to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "        predictions = torch.sigmoid(outputs).cpu().numpy()\n",
    "\n",
    "        # Sellect top-25 values as predictions\n",
    "        top_25 = np.argsort(-predictions, axis=1)[:, :25]\n",
    "        if top_k_indices is None:\n",
    "            top_k_indices = top_25\n",
    "        else:\n",
    "            top_k_indices = np.concatenate((top_k_indices, top_25), axis=0)\n",
    "\n",
    "        surveys.extend(surveyID.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d476e",
   "metadata": {
    "papermill": {
     "duration": 0.01651,
     "end_time": "2024-05-05T21:21:33.076908",
     "exception": false,
     "start_time": "2024-05-05T21:21:33.060398",
     "status": "completed"
    },
    "tags": []
   },
   "source": "## Save prediction file! ðŸŽ‰ðŸ¥³ðŸ™ŒðŸ¤—"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e027600",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T20:41:05.982840Z",
     "iopub.status.busy": "2025-03-13T20:41:05.982564Z",
     "iopub.status.idle": "2025-03-13T20:41:06.153832Z",
     "shell.execute_reply": "2025-03-13T20:41:06.153240Z",
     "shell.execute_reply.started": "2025-03-13T20:41:05.982816Z"
    },
    "papermill": {
     "duration": 0.124413,
     "end_time": "2024-05-05T21:21:33.218031",
     "exception": false,
     "start_time": "2024-05-05T21:21:33.093618",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_concatenated = [\" \".join(map(str, row)) for row in top_k_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"surveyId\": surveys,\n",
    "        \"predictions\": data_concatenated,\n",
    "    }\n",
    ").to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11364227,
     "sourceId": 91196,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1239.673699,
   "end_time": "2024-05-05T21:21:35.811865",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-05T21:00:56.138166",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}